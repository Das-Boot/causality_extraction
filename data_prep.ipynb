{"cells":[{"cell_type":"markdown","source":"<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Parse-XML-Data\" data-toc-modified-id=\"Parse-XML-Data-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Parse XML Data</a></span></li><li><span><a href=\"#Tokenize-Text\" data-toc-modified-id=\"Tokenize-Text-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Tokenize Text</a></span></li><li><span><a href=\"#Create-Word-Embeddings-with-Wiki-Extvec\" data-toc-modified-id=\"Create-Word-Embeddings-with-Wiki-Extvec-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Create Word Embeddings with Wiki-Extvec</a></span></li><li><span><a href=\"#Create-Contextual-String-Embeddings-with-Flair\" data-toc-modified-id=\"Create-Contextual-String-Embeddings-with-Flair-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Create Contextual String Embeddings with Flair</a></span></li><li><span><a href=\"#Create-BERT-Embeddings-with-flair\" data-toc-modified-id=\"Create-BERT-Embeddings-with-flair-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Create BERT Embeddings with flair</a></span></li><li><span><a href=\"#Create-ELMo-Embeddings-with-flair\" data-toc-modified-id=\"Create-ELMo-Embeddings-with-flair-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Create ELMo Embeddings with flair</a></span></li><li><span><a href=\"#Build-Dataset\" data-toc-modified-id=\"Build-Dataset-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Build Dataset</a></span><ul class=\"toc-item\"><li><span><a href=\"#Tagging\" data-toc-modified-id=\"Tagging-7.1\"><span class=\"toc-item-num\">7.1&nbsp;&nbsp;</span>Tagging</a></span></li><li><span><a href=\"#Split-Dataset\" data-toc-modified-id=\"Split-Dataset-7.2\"><span class=\"toc-item-num\">7.2&nbsp;&nbsp;</span>Split Dataset</a></span></li></ul></li><li><span><a href=\"#Data-compress\" data-toc-modified-id=\"Data-compress-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Data compress</a></span></li></ul></div>","metadata":{"deepnote_cell_type":"markdown","toc":true,"cell_id":"00000-88fc801b-cc10-4c90-a50c-59110e07f396"}},{"cell_type":"markdown","source":"# Parse XML Data","metadata":{"deepnote_cell_type":"markdown","ExecuteTime":{"end_time":"2019-01-30T07:21:57.478760Z","start_time":"2019-01-30T07:21:57.474162Z"},"cell_id":"00001-ee767a82-05ff-4481-af3d-4e61466d6b44"}},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","ExecuteTime":{"end_time":"2019-09-13T14:40:51.454360Z","start_time":"2019-09-13T14:40:51.263265Z"},"cell_id":"00002-a4dae8f0-c4a3-4827-807c-8f45370e6efa"},"source":"from xml.dom.minidom import parse\nimport xml.dom.minidom\nfrom collections import Counter\n\n\ndef parse(n):\n    \"\"\"\n    Parse XML corpus\n    \"\"\"\n    sentence, label = [], []\n    DOMTree = xml.dom.minidom.parse(n)\n    items = DOMTree.documentElement.getElementsByTagName('item')\n    for item in items:\n        label.append(item.getAttribute('label'))\n        sent = item.getElementsByTagName('sentence')\n        sentence.append(sent[0].childNodes[0].data)\n    return sentence, label\n\n\ntrainSent, trainLabel = parse('corpus/train-corpus.xml')\ntestSent, testLabel = parse('corpus/test-corpus.xml')","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Tokenize Text","metadata":{"deepnote_cell_type":"markdown","ExecuteTime":{"end_time":"2019-01-30T07:23:06.567214Z","start_time":"2019-01-30T07:23:06.562482Z"},"cell_id":"00003-051e1ba2-c9fb-4e51-8a69-889133a84315"}},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","ExecuteTime":{"end_time":"2019-09-13T14:41:03.578566Z","start_time":"2019-09-13T14:41:01.837726Z"},"cell_id":"00004-e5a555c8-305e-4517-bd8d-6f538f5011e1"},"source":"from keras.preprocessing.text import text_to_word_sequence\nimport numpy as np\nimport pickle\nimport re\nMAX_WLEN = 58\nMAX_CLEN = 23\n\n\ndef delete(s):\n    \"\"\"\n    Delete parentheses\n    \"\"\"\n    return re.sub('\\((.*?)\\)', '', s)\n\n\ndef cut(s):\n    \"\"\"\n    Word segmentation\n    \"\"\"\n    ws = text_to_word_sequence(s,\n                               filters='!#$&*+.%/<=>?@[\\\\]^_`{|}~\\t\\n',\n                               lower=False,\n                               split=' ')\n    for s in [',', ':', ';', '\"']:\n        sep_pm(ws, s)\n    return ws\n\n\ndef sep_pm(ws, s):\n    \"\"\"\n    Separate punctuation mark: , : ; \" \n    \"\"\"\n    if s in [',', ':', ';', '\"']:\n        for i in range(len(ws)):\n            if ws[i].endswith(s) and ws[i] != s:\n                ws[i] = ws[i][:-1]\n                ws.insert(i+1, s)\n        for i in range(len(ws)):\n            if ws[i].endswith(s) and ws[i] != s:\n                ws[i] = ws[i][:-1]\n                ws.insert(i+1, s)\n    if s in ['\"']:\n        for i in range(len(ws)):\n            if ws[i].startswith(s) and ws[i] != s:\n                ws[i] = ws[i][1:]\n                ws.insert(i, s)\n        for i in range(len(ws)):\n            if ws[i].startswith(s) and ws[i] != s:\n                ws[i] = ws[i][1:]\n                ws.insert(i, s)\n\n\ndef find_cp_idx(ws):\n    \"\"\"\n    Find index of causality phrases\n    \"\"\"\n    E = []\n    for n in ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11']:\n        e = []\n        for i in range(len(ws)):\n            if ws[i] == 'e'+n:\n                e.append(i-2*(int(n)-1))\n                for j in range(i+1, len(ws)):\n                    if ws[j] == 'e'+n and j-2 != i:\n                        e.append(j-2*int(n))\n                break\n        if e != []:\n            E.append([i for i in range(e[0], e[-1]+1)])\n    return E\n\n\ndef format_sentence(ws):\n    \"\"\"\n    Remove entity tags\n    \"\"\"\n    return [i for i in ws if i not in ['e1', 'e2', 'e3', 'e4', 'e5', 'e6', 'e7', 'e8', 'e9', 'e10', 'e11']]\n\n\ntrain_index = []\ntrainSent = [delete(i) for i in trainSent]\ntrainWords = [cut(i) for i in trainSent]\nfor w in trainWords:\n    train_index.append(find_cp_idx(w))\ntrainWords = [format_sentence(i) for i in trainWords]\ntrainChars = [[list(w) for w in s] for s in trainWords]\n\ntest_index = []\ntestSent = [delete(i) for i in testSent]\ntestWords = [cut(i) for i in testSent]\nfor w in testWords:\n    test_index.append(find_cp_idx(w))\ntestWords = [format_sentence(i) for i in testWords]\ntestChars = [[list(w) for w in s] for s in testWords]\n\ncounts = Counter()\nfor sw in trainWords+testWords:\n    counts.update(sw)\n\nvocab = sorted(counts, key=counts.get, reverse=True)\nword2index = {w: i for i, w in enumerate(vocab, 1)}\nindex2word = {i: w for w, i in word2index.items()}\n\ncounts = Counter()\nfor sc in trainChars+testChars:\n    counts.update(sum(sc, []))\n\nc_vocab = sorted(counts, key=counts.get, reverse=True)\nchar2index = {w: i for i, w in enumerate(c_vocab, 1)}\nindex2char = {i: w for w, i in char2index.items()}\n\nw_filePath = 'data/index/index_w.pkl'\nwith open(w_filePath, 'wb') as fp:\n    pickle.dump((word2index, index2word), fp, -1)\n\nc_filePath = 'data/index/index_c.pkl'\nwith open(c_filePath, 'wb') as fp:\n    pickle.dump((char2index, index2char), fp, -1)","execution_count":null,"outputs":[{"name":"stderr","output_type":"stream","text":"/home/lizhn7/anaconda/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n  from ._conv import register_converters as _register_converters\nUsing TensorFlow backend.\n"}]},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","ExecuteTime":{"end_time":"2019-09-13T14:41:03.599546Z","start_time":"2019-09-13T14:41:03.580711Z"},"cell_id":"00005-6e9472f1-b1d6-41f1-aa67-de8aaf286ad3"},"source":"causalLabel = [l for l in testLabel if l != 'Non-Causal']\nprint('\\n------ Causal Sentence Analysis ------\\n')\nprint('{:<40}{:>30}\\n'.format('', 'number of causality triplet'))\nfor l in ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12']:\n    print('{:<40}{:>30d}'.format(\n        l+':', Counter([Counter(i[12:])['e']/2 for i in causalLabel])[int(l)]))\nprint('{:<40}{:>30d}'.format('total', int(\n    sum([Counter(i[12:])['e']/2 for i in causalLabel]))))","execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":"\n------ Causal Sentence Analysis ------\n\n                                           number of causality triplet\n\n1:                                                                 126\n2:                                                                  47\n3:                                                                   7\n4:                                                                   8\n5:                                                                   1\n6:                                                                   1\n7:                                                                   0\n8:                                                                   0\n9:                                                                   0\n10:                                                                  0\n11:                                                                  0\n12:                                                                  1\ntotal                                                              296\n"}]},{"cell_type":"markdown","source":"# Create Word Embeddings with Wiki-Extvec","metadata":{"deepnote_cell_type":"markdown","heading_collapsed":true,"cell_id":"00006-359d41dd-7f9e-46e2-bcd9-ebc1f08066d2"}},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","ExecuteTime":{"end_time":"2019-08-16T06:08:13.787182Z","start_time":"2019-08-16T06:08:12.197528Z"},"hidden":true,"cell_id":"00007-ef9267d1-77ce-40a1-98ab-4225c0f7c5e2"},"source":"from pynlp import StanfordCoreNLP\nfrom nltk import WordNetLemmatizer, PorterStemmer, LancasterStemmer\nimport spacy\nimport math\nimport h5py","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","ExecuteTime":{"end_time":"2019-08-16T06:08:52.260980Z","start_time":"2019-08-16T06:08:36.738969Z"},"hidden":true,"cell_id":"00008-97167ae4-d3d9-4ede-bffb-b7edb550ba32"},"source":"annotators = 'lemma'\ncore_nlp = StanfordCoreNLP(annotators=annotators)\nnlp = spacy.load('en_3')\nwnl = WordNetLemmatizer()\nporter = PorterStemmer()\nlancaster = LancasterStemmer()\n\nVOCAB_SIZE = len(word2index)+1\nEXTVEC_DIM = 300","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","ExecuteTime":{"end_time":"2019-08-16T06:10:36.086162Z","start_time":"2019-08-16T06:10:36.073670Z"},"hidden":true,"cell_id":"00009-62b32e0b-c003-4e92-aa1a-eeee292cdda4"},"source":"def replace(w, d):\n    \"\"\"\n    Replace the words that are not in the dictionary\n    \"\"\"\n    r = d.get(w)\n    if r is None:\n        nw = w.lower()\n        r = d.get(nw)\n    if r is None:\n        nw = [i[0].lemma for i in core_nlp(w)][0]\n        r = d.get(nw)\n    if r is None:\n        nw = [i.lemma_ for i in nlp(w)][0]\n        r = d.get(nw)\n    if r is None:\n        nw = wnl.lemmatize(w)\n        r = d.get(nw)\n    if r is None:\n        nw = porter.stem(w)\n        r = d.get(nw)\n    if r is None:\n        nw = lancaster.stem(w)\n        r = d.get(nw)\n    if r is None:\n        nw = w[:-1]\n        r = d.get(nw)\n    if r is None:\n        nw = w[:-2]\n        r = d.get(nw)\n    return r","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","ExecuteTime":{"end_time":"2019-08-16T06:04:29.900025Z","start_time":"2019-08-16T06:01:40.576668Z"},"hidden":true,"cell_id":"00010-2466fcbd-5386-4302-b009-572d28573f57"},"source":"extvec_n_symbols = 1476022\nextvec_index_dict = {}\nextvec_embedding_weights = np.empty((extvec_n_symbols, EXTVEC_DIM))\nfilePath = 'your path to /dependency-based_word_embeddings/wiki_extvec'\nwith open(filePath, encoding='utf-8') as fp:\n    index = 0\n    for l in fp:\n        l = l.split(' ')\n        word = l[0]\n        extvec_index_dict[word] = index\n        extvec_embedding_weights[index, :] = np.asarray(l[1:], dtype='float32')\n        index += 1","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","ExecuteTime":{"end_time":"2019-08-16T06:10:42.992629Z","start_time":"2019-08-16T06:10:42.874544Z"},"hidden":true,"cell_id":"00011-0387f193-5e70-4c31-a312-b198ac57b8d0"},"source":"# Generate random embedding with same scale as extvec\nSEED = 666\nnp.random.seed(SEED)\nshape = (VOCAB_SIZE, EXTVEC_DIM)\nscale = math.sqrt(3.0 / EXTVEC_DIM)\nextvec_embedding = np.random.uniform(low=-scale, high=scale, size=shape)","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","ExecuteTime":{"end_time":"2019-08-16T06:16:54.527740Z","start_time":"2019-08-16T06:16:34.487785Z"},"hidden":true,"cell_id":"00012-566ab896-d076-48a7-b337-e8cfa8f2a255"},"source":"# Copy from extvec weights of words that appear in index2word\ncount = 0\nfor i in range(1, VOCAB_SIZE):\n    w = index2word[i]\n    g = extvec_index_dict.get(w)\n    if g is None:\n        g = replace(w, extvec_index_dict)\n    if g is not None:\n        extvec_embedding[i, :] = extvec_embedding_weights[g, :]\n        count += 1\nprint('{num_tokens}-{per:.3f}% tokens in vocab found in Wiki-Extvec and copied to embedding.'.format(\n    num_tokens=count, per=count/float(VOCAB_SIZE)*100))","execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":"15129-97.361% tokens in vocab found in Wiki-Extvec and copied to embedding.\n"}]},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","ExecuteTime":{"end_time":"2019-08-16T06:17:40.831890Z","start_time":"2019-08-16T06:17:40.669462Z"},"hidden":true,"cell_id":"00013-36e55563-3064-458e-85da-7f827273a9cc"},"source":"filePath = 'data/embedding/extvec_embedding.npy'\nnp.save(open(filePath, 'wb'), extvec_embedding)","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create Contextual String Embeddings with Flair","metadata":{"deepnote_cell_type":"markdown","heading_collapsed":true,"cell_id":"00014-d0a2f7e7-5c26-4f8f-92d1-1c7206cc7853"}},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","ExecuteTime":{"end_time":"2019-08-15T16:54:22.149215Z","start_time":"2019-08-15T16:54:22.138565Z"},"hidden":true,"cell_id":"00015-89580b53-6847-41f0-bcde-16e816801064"},"source":"from flair.embeddings import FlairEmbeddings, StackedEmbeddings\nfrom tqdm import tqdm\nfrom flair.data import Sentence\nFLAIR_DIM = 4096\n\n\ndef flair_cse(sw):\n    \"\"\"\n    Convert sentence to contextual string embeddings with flair\n    \"\"\"\n    charlm_embedding_forward = FlairEmbeddings('news-forward')\n    charlm_embedding_backward = FlairEmbeddings('your path to /news-backward-0.4.1.pt')\n    stacked_embeddings = StackedEmbeddings(\n        embeddings=[charlm_embedding_forward, charlm_embedding_backward])\n    result = []\n    nsw = [Sentence(' '.join(i)) for i in sw]\n    for s in tqdm(nsw):\n        stacked_embeddings.embed(s)\n        result.append(np.concatenate((np.array([np.array(\n            token.embedding) for token in s]), np.zeros((MAX_WLEN-len(s), FLAIR_DIM))), axis=0))\n    return np.array(result)","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","ExecuteTime":{"end_time":"2019-08-15T17:35:40.708248Z","start_time":"2019-08-15T16:54:23.309924Z"},"hidden":true,"cell_id":"00016-0da9be2e-57a3-40b7-a1af-f67870d024d3"},"source":"trainFlair = flair_cse(trainWords)","execution_count":null,"outputs":[{"name":"stderr","output_type":"stream","text":"100%|██████████| 4450/4450 [40:58<00:00,  1.78it/s] \n"}]},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","ExecuteTime":{"end_time":"2019-08-15T17:43:12.749957Z","start_time":"2019-08-15T17:35:40.883766Z"},"hidden":true,"cell_id":"00017-ba949e43-3dcf-4c1a-80d8-8e7f6b76e8f1"},"source":"testFlair = flair_cse(testWords)","execution_count":null,"outputs":[{"name":"stderr","output_type":"stream","text":"100%|██████████| 786/786 [07:30<00:00,  1.33it/s]\n"}]},{"cell_type":"markdown","source":"# Create BERT Embeddings with flair","metadata":{"deepnote_cell_type":"markdown","cell_id":"00018-bb91e0e3-edd2-40ad-a40d-2bd87f0f6217"}},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","ExecuteTime":{"end_time":"2019-08-16T01:29:56.760032Z","start_time":"2019-08-16T01:29:55.479793Z"},"cell_id":"00019-200edb80-9269-44cd-87fa-8b6679e3cb3f"},"source":"from flair.embeddings import BertEmbeddings\nfrom tqdm import tqdm\nfrom flair.data import Sentence\nBERT_DIM = 4096\n\n\ndef flair_bert(sw):\n    \"\"\"\n    Convert sentence to bert embeddings with flair\n    \"\"\"\n    bert_embedding = BertEmbeddings('bert-large-cased')\n    result = []\n    nsw = [Sentence(' '.join(i)) for i in sw]\n    for s in tqdm(nsw):\n        bert_embedding.embed(s)\n        result.append(np.concatenate((np.array([np.array(\n            token.embedding) for token in s]), np.zeros((MAX_WLEN-len(s), BERT_DIM))), axis=0))\n    return np.array(result)","execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":"Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n"}]},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","ExecuteTime":{"end_time":"2019-08-16T01:56:05.012683Z","start_time":"2019-08-16T01:30:00.593846Z"},"cell_id":"00020-d7a4f020-69d0-40c4-a0ed-6c86797ee613"},"source":"trainBERT = flair_bert(trainWords)","execution_count":null,"outputs":[{"name":"stderr","output_type":"stream","text":"100%|██████████| 4450/4450 [25:15<00:00,  2.72it/s]\n"}]},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","ExecuteTime":{"end_time":"2019-08-16T02:06:33.102788Z","start_time":"2019-08-16T02:01:47.330643Z"},"cell_id":"00021-4a0338d5-2d8e-473e-872e-de838454d568"},"source":"testBERT = flair_bert(testWords)","execution_count":null,"outputs":[{"name":"stderr","output_type":"stream","text":"100%|██████████| 786/786 [04:18<00:00,  2.72it/s]\n"}]},{"cell_type":"markdown","source":"# Create ELMo Embeddings with flair","metadata":{"deepnote_cell_type":"markdown","heading_collapsed":true,"cell_id":"00022-32cef4bb-9526-483e-bebd-2ce4fea5f6ac"}},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","ExecuteTime":{"end_time":"2019-08-15T18:51:30.753766Z","start_time":"2019-08-15T18:51:29.128995Z"},"hidden":true,"cell_id":"00023-e62f704f-4e82-4903-bb64-ccdf8f5611a8"},"source":"from flair.embeddings import ELMoEmbeddings\nfrom tqdm import tqdm\nfrom flair.data import Sentence\nELMO_DIM = 3072\n\n\ndef flair_elmo(sw):\n    \"\"\"\n    Convert sentence to ELMo embeddings with flair\n    \"\"\"\n    elmo_embedding = ELMoEmbeddings('original')\n    result = []\n    nsw = [Sentence(' '.join(i)) for i in sw]\n    for s in tqdm(nsw):\n        elmo_embedding.embed(s)\n        result.append(np.concatenate((np.array([np.array(\n            token.embedding) for token in s]), np.zeros((MAX_WLEN-len(s), ELMO_DIM))), axis=0))\n    return np.array(result)","execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":"Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n"}]},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","ExecuteTime":{"end_time":"2019-08-15T19:27:12.541378Z","start_time":"2019-08-15T18:51:37.530066Z"},"hidden":true,"cell_id":"00024-9abe9133-1098-43fc-8c4e-c61db98dd825"},"source":"trainELMo = flair_elmo(trainWords)","execution_count":null,"outputs":[{"name":"stderr","output_type":"stream","text":"100%|██████████| 4450/4450 [26:45<00:00,  2.58it/s]\n"}]},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","ExecuteTime":{"end_time":"2019-08-15T19:33:23.864448Z","start_time":"2019-08-15T19:27:13.015289Z"},"hidden":true,"cell_id":"00025-0bbc2f79-e864-459a-83f8-6767e56b4b01"},"source":"testELMo = flair_elmo(testWords)","execution_count":null,"outputs":[{"name":"stderr","output_type":"stream","text":"100%|██████████| 786/786 [04:34<00:00,  2.14it/s]\n"}]},{"cell_type":"markdown","source":"# Build Dataset","metadata":{"deepnote_cell_type":"markdown","cell_id":"00026-ba4e6081-2d4e-4e45-ac9f-55662268d19b"}},{"cell_type":"markdown","source":"## Tagging","metadata":{"deepnote_cell_type":"markdown","cell_id":"00027-a50e6fb3-421b-4674-aec8-4cf5e2d777b3"}},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","ExecuteTime":{"end_time":"2019-09-13T14:41:17.470972Z","start_time":"2019-09-13T14:41:17.452310Z"},"cell_id":"00028-eae31611-8f10-46df-bdc0-cb6b867f0b04"},"source":"def find_cp(l):\n    \"\"\"\n    Find index of cause and effect from label\n    \"\"\"\n    p = '\\((e.*?)\\)'\n    t = re.findall(p, l)\n    t = [i.split(',') for i in t]\n    c = [i[0] for i in t]\n    e = [i[-1] for i in t]\n    return c, e\n\n\ndef tagging(s, n, w):\n    \"\"\"\n    Tagging\n    \"\"\"\n    for i in range(len(w)):\n        if i == 0:\n            s[w[i]] = n\n        else:\n            s[w[i]] = n+1\n    return s\n\n\ndef word2label(s, idx, l):\n    \"\"\"\n    Convert sentence to label sequence\n    :Tagging Scheme\n    ---7 Tag---\n    0: O = Other\n    1: B-C = Cause Begin\n    2: I-C = Cause Inside\n    3: B-E = Effect Begin\n    4: I-E = Effect Inside\n    5: B-CE = Cause|Effect Begin\n    6: I-CE = Cause|Effect Inside\n    \"\"\"\n    r = [0]*len(s)\n    if l == 'Non-Causal':\n        return r\n    else:\n        c, e = find_cp(l)\n        element = set(c+e)\n        cause, cause_and_effect, effect = [], [], []\n        for i in element:\n            if i not in e:\n                cause.append(i)\n            if i in c and i in e:\n                cause_and_effect.append(i)\n            if i not in c and i in e:\n                effect.append(i)\n        for e in element:\n            if e in cause:\n                r = tagging(r, 1, idx[int(e[1:])-1])\n            if e in effect:\n                r = tagging(r, 3, idx[int(e[1:])-1])\n            if e in cause_and_effect:\n                r = tagging(r, 5, idx[int(e[1:])-1])\n    return r\n\n\ntrain_labelSeq = [word2label(\n    trainWords[i], train_index[i], trainLabel[i]) for i in range(len(trainWords))]\ntest_labelSeq = [word2label(testWords[i], test_index[i], testLabel[i])\n                 for i in range(len(testWords))]","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","ExecuteTime":{"end_time":"2019-09-13T14:41:19.108721Z","start_time":"2019-09-13T14:41:18.211470Z"},"cell_id":"00029-d49e1bef-c798-42a2-abeb-58f37cc6f2c2"},"source":"labelSeq = sum(train_labelSeq+test_labelSeq, [])\ntagAll = ['O', 'B-C', 'I-C', 'B-E', 'I-E', 'B-CE', 'I-CE']\nprint('\\n------ Corpous Tag Analysis ------\\n')\nprint('{:<20}{:>30}{:>30}\\n'.format('', 'number of tag', 'percentage'))\nfor i in range(len(tagAll)):\n    print('{:<20}{:>30d}{:>30}'.format(tagAll[i]+':', Counter(labelSeq)[i], str(Counter(\n        labelSeq)[i]/sum(Counter(labelSeq)[i] for i in range(len(tagAll)))*100)[:5]+'%'))\nprint('{:<20}{:>30d}{:>30d}'.format('total', sum(\n    Counter(labelSeq)[i] for i in range(len(tagAll))), 1))","execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":"\n------ Corpous Tag Analysis ------\n\n                                     number of tag                    percentage\n\nO:                                           78440                        92.57%\nB-C:                                          1544                        1.822%\nI-C:                                          1650                        1.947%\nB-E:                                          1506                        1.777%\nI-E:                                          1460                        1.723%\nB-CE:                                           64                        0.075%\nI-CE:                                           71                        0.083%\ntotal                                        84735                             1\n"}]},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","ExecuteTime":{"end_time":"2019-09-13T14:41:21.034482Z","start_time":"2019-09-13T14:41:20.399804Z"},"cell_id":"00030-8dc7f86d-c250-47d3-9161-2d7f4205a31c"},"source":"labelSeq = sum(train_labelSeq, [])\ntagAll = ['O', 'B-C', 'I-C', 'B-E', 'I-E', 'B-CE', 'I-CE']\nprint('\\n------ Corpous Tag Analysis ------\\n')\nprint('{:<20}{:>30}{:>30}\\n'.format('', 'number of tag', 'percentage'))\nfor i in range(len(tagAll)):\n    print('{:<20}{:>30d}{:>30}'.format(tagAll[i]+':', Counter(labelSeq)[i], str(Counter(\n        labelSeq)[i]/sum(Counter(labelSeq)[i] for i in range(len(tagAll)))*100)[:5]+'%'))\nprint('{:<20}{:>30d}{:>30d}'.format('total', sum(\n    Counter(labelSeq)[i] for i in range(len(tagAll))), 1))","execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":"\n------ Corpous Tag Analysis ------\n\n                                     number of tag                    percentage\n\nO:                                           66614                        92.58%\nB-C:                                          1308                        1.817%\nI-C:                                          1421                        1.974%\nB-E:                                          1268                        1.762%\nI-E:                                          1230                        1.709%\nB-CE:                                           55                        0.076%\nI-CE:                                           55                        0.076%\ntotal                                        71951                             1\n"}]},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","ExecuteTime":{"end_time":"2019-09-12T12:27:52.944230Z","start_time":"2019-09-12T12:27:52.866713Z"},"cell_id":"00033-71791505-975e-49df-beb3-5b62f1d12024"},"source":"labelSeq = sum(test_labelSeq, [])\ntagAll = ['O', 'B-C', 'I-C', 'B-E', 'I-E', 'B-CE', 'I-CE']\nprint('\\n------ Corpous Tag Analysis ------\\n')\nprint('{:<20}{:>30}{:>30}\\n'.format('', 'number of tag', 'percentage'))\nfor i in range(len(tagAll)):\n    print('{:<20}{:>30d}{:>30}'.format(tagAll[i]+':', Counter(labelSeq)[i], str(Counter(\n        labelSeq)[i]/sum(Counter(labelSeq)[i] for i in range(len(tagAll)))*100)[:5]+'%'))\nprint('{:<20}{:>30d}{:>30d}'.format('total', sum(\n    Counter(labelSeq)[i] for i in range(len(tagAll))), 1))","execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":"\n------ Corpous Tag Analysis ------\n\n                                     number of tag                    percentage\n\nO:                                           11826                        92.50%\nB-C:                                           236                        1.846%\nI-C:                                           229                        1.791%\nB-E:                                           238                        1.861%\nI-E:                                           230                        1.799%\nB-CE:                                            9                        0.070%\nI-CE:                                           16                        0.125%\ntotal                                        12784                             1\n"}]},{"cell_type":"markdown","source":"## Split Dataset","metadata":{"deepnote_cell_type":"markdown","cell_id":"00046-2c7d0c45-f3e5-4e15-ad46-65310c76d20d"}},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","ExecuteTime":{"end_time":"2019-09-08T08:20:16.584544Z","start_time":"2019-09-08T08:20:15.971640Z"},"cell_id":"00047-2ebc934a-97ab-49c2-92ea-5bf26c0549f8"},"source":"from keras.preprocessing.sequence import pad_sequences\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nimport h5py\nSEED = 666\n\n\ndef char_data(cw):\n    result = []\n    for s in cw:\n        result.append(np.concatenate((pad_sequences([[char2index[c] for c in w] for w in s], maxlen=MAX_CLEN, padding='post', truncating='post'),\n                                      np.zeros((MAX_WLEN-len(s), MAX_CLEN))), axis=0))\n    return np.array(result)\n\n\ntrainCrray = char_data(trainChars)\n\ntestCrray = char_data(testChars)\n\ntrainSeq = [[word2index[cw] for cw in s] for s in trainWords]\ntrainWrray = pad_sequences(trainSeq, maxlen=MAX_WLEN,\n                           padding='post', truncating='post')\n\ntestSeq = [[word2index[cw] for cw in s] for s in testWords]\ntestWrray = pad_sequences(testSeq, maxlen=MAX_WLEN,\n                          padding='post', truncating='post')\n\ntrain_labelSeq = pad_sequences(\n    train_labelSeq, maxlen=MAX_WLEN, padding='post', truncating='post')\ntrain_labelData = train_labelSeq.reshape(len(trainWords), MAX_WLEN, 1)\n\ntest_labelSeq = pad_sequences(\n    test_labelSeq, maxlen=MAX_WLEN, padding='post', truncating='post')\ntest_labelData = test_labelSeq.reshape(len(testWords), MAX_WLEN, 1)\n\nxTrain, _, yTrain, _ = train_test_split(\n    trainWrray, train_labelData, test_size=0., random_state=SEED)\nxTest, _, yTest, _ = train_test_split(\n    testWrray, test_labelData, test_size=0., random_state=SEED)\nxTrain_c = train_test_split(trainCrray, test_size=0., random_state=SEED)[0]\nxTest_c = train_test_split(testCrray, test_size=0., random_state=SEED)[0]\nxTrain_flair = train_test_split(trainFlair, test_size=0., random_state=SEED)[0]\nxTest_flair = train_test_split(testFlair, test_size=0., random_state=SEED)[0]\nxTrain_bert = train_test_split(trainBERT, test_size=0., random_state=SEED)[0]\nxTest_bert = train_test_split(testBERT, test_size=0., random_state=SEED)[0]\nxTrain_elmo = train_test_split(trainELMo, test_size=0., random_state=SEED)[0]\nxTest_elmo = train_test_split(testELMo, test_size=0., random_state=SEED)[0]","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","ExecuteTime":{"end_time":"2019-09-10T13:45:43.715341Z","start_time":"2019-09-10T13:45:43.708724Z"},"cell_id":"00049-3d69c1c1-6afd-4a34-8902-b761010aaca5"},"source":"xTrain.shape, yTrain.shape, xTest.shape, yTest.shape, xTrain_c.shape, xTest_c.shape, xTrain_flair.shape, xTest_flair.shape","execution_count":null,"outputs":[{"data":{"text/plain":"((4501, 58),\n (4501, 58, 1),\n (786, 58),\n (786, 58, 1),\n (4501, 58, 23),\n (786, 58, 23),\n (4501, 58, 4096),\n (786, 58, 4096))"},"execution_count":22,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","ExecuteTime":{"end_time":"2019-08-15T17:49:37.076801Z","start_time":"2019-08-15T17:49:10.319158Z"},"cell_id":"00058-b0f160aa-a51c-43ec-81fb-8c28fe5f402b"},"source":"h5f = h5py.File('data/train/train.h5', 'w')\nh5f.create_dataset('xTrain', data=xTrain)\nh5f.create_dataset('xTrain_c', data=xTrain_c)\nh5f.create_dataset('yTrain', data=yTrain)\nh5f.close()\n\nh5f = h5py.File('data/test/test.h5', 'w')\nh5f.create_dataset('xTest', data=xTest)\nh5f.create_dataset('xTest_c', data=xTest_c)\nh5f.create_dataset('yTest', data=yTest)\nh5f.close()\n\nh5f = h5py.File('data/embedding/flair.h5', 'w')\nh5f.create_dataset('xTrain_flair', data=xTrain_flair)\nh5f.create_dataset('xTest_flair', data=xTest_flair)\nh5f.close()","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","ExecuteTime":{"end_time":"2019-08-16T02:14:15.161040Z","start_time":"2019-08-16T02:13:44.930671Z"},"cell_id":"00059-651ebd8b-4ad9-47d9-beab-49fd96478d42"},"source":"h5f = h5py.File('data/embedding/bert.h5', 'w')\nh5f.create_dataset('xTrain_bert', data=xTrain_bert)\nh5f.create_dataset('xTest_bert', data=xTest_bert)\nh5f.close()","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","ExecuteTime":{"end_time":"2019-08-15T19:34:08.103928Z","start_time":"2019-08-15T19:33:47.434196Z"},"cell_id":"00060-45c8548d-ba2e-4291-b879-74d4ce5cf2c3"},"source":"h5f = h5py.File('data/embedding/elmo.h5', 'w')\nh5f.create_dataset('xTrain_elmo', data=xTrain_elmo)\nh5f.create_dataset('xTest_elmo', data=xTest_elmo)\nh5f.close()","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data compress","metadata":{"deepnote_cell_type":"markdown","cell_id":"00061-e5d91ae5-a48a-47a8-9847-03f100cf6e14"}},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","ExecuteTime":{"end_time":"2019-09-11T02:07:06.547811Z","start_time":"2019-09-11T02:07:06.542672Z"},"cell_id":"00062-2d65ba8c-91ff-4803-8c60-8f66aa150292"},"source":"import zlib\n\ndef compress(infile, dst, level=9):\n    infile = open(infile, 'rb')\n    dst = open(dst, 'wb')\n    compress = zlib.compressobj(level)\n    data = infile.read(1024)\n    while data:\n        dst.write(compress.compress(data))\n        data = infile.read(1024)\n    dst.write(compress.flush())\n\n\ndef decompress(infile, dst):\n    infile = open(infile, 'rb')\n    dst = open(dst, 'wb')\n    decompress = zlib.decompressobj()\n    data = infile.read(1024)\n    while data:\n        dst.write(decompress.decompress(data))\n        data = infile.read(1024)\n    dst.write(decompress.flush())\n    \n#compress('data/embedding/bert.h5',\n#         'data/embedding/c_bert.h5')","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","ExecuteTime":{"end_time":"2019-09-11T02:07:14.085105Z","start_time":"2019-09-11T02:07:07.217458Z"},"cell_id":"00063-77595f94-e17e-4e05-bd02-45c3caf79270"},"source":"decompress('data/embedding/c_bert.h5', 'data/embedding/bert.h5')","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","ExecuteTime":{"end_time":"2019-09-06T08:01:43.611348Z","start_time":"2019-09-06T08:01:19.991285Z"},"cell_id":"00064-32912954-885d-4972-8871-d6ebad5dc2d0"},"source":"decompress('data/embedding/c_elmo.h5', 'data/embedding/elmo.h5')","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","ExecuteTime":{"end_time":"2019-08-16T10:36:50.247301Z","start_time":"2019-08-16T10:36:10.674948Z"},"cell_id":"00065-c7c0ddf1-2d92-4576-8f45-00ec342ca3f7"},"source":"decompress('data/embedding/c_flair.h5', 'data/embedding/flair.h5')","execution_count":null,"outputs":[]}],"nbformat":4,"nbformat_minor":2,"metadata":{"hide_input":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":true,"toc_position":{"height":"calc(100% - 180px)","left":"10px","top":"150px","width":"384px"},"toc_section_display":true,"toc_window_display":false},"varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"window_display":false},"deepnote_notebook_id":"43b5dbd4-b093-41c1-8cb0-b0188e5a28b2","deepnote_execution_queue":[]}}